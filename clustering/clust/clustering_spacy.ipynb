{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Error assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-73443e565879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **overrides)\u001b[0m\n\u001b[1;32m    277\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagger'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                          \u001b[0;32mif\u001b[0m \u001b[0;34m'parser'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_parser\u001b[0;34m(cls, nlp, **cfg)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'deps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/spacy/syntax/parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.parser.Parser.load (spacy/syntax/parser.cpp:7659)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/thinc/linear/avgtron.pyx\u001b[0m in \u001b[0;36mthinc.linear.avgtron.AveragedPerceptron.load (thinc/linear/avgtron.cpp:3514)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/preshed/maps.pyx\u001b[0m in \u001b[0;36mpreshed.maps.PreshMap.__init__ (preshed/maps.cpp:1193)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/preshed/maps.pyx\u001b[0m in \u001b[0;36mpreshed.maps.map_init (preshed/maps.cpp:3058)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documentos/facu/mdt/venv/lib/python3.6/site-packages/cymem/cymem.pyx\u001b[0m in \u001b[0;36mcymem.cymem.Pool.alloc (cymem/cymem.cpp:1091)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Error assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytesError assigning 16777216 bytes"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pbaspacy.txt\", \"r\") as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "raw = tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpiando\n",
    "clean = []\n",
    "for sent in raw:\n",
    "    a = re.sub(r'&#\\w+;', '', sent)\n",
    "    b = re.sub(r'[^\\w]', ' ', a)\n",
    "    c = b.rstrip().lstrip()\n",
    "    clean.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,a in enumerate(clean[:27]):\n",
    "    print(str(i)+\")\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para sacar las letras que quedaron colgadas al principio\n",
    "last_clean = []\n",
    "for a in clean:\n",
    "    aux = a.split()\n",
    "    if len(aux[0]) == 1:\n",
    "        if aux[0] not in {'A', 'Y', 'O'}:\n",
    "            del aux[0]\n",
    "    last_clean.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,a in enumerate(last_clean[:5]):\n",
    "    print(str(i)+\")\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten list\n",
    "flat = []\n",
    "for s in last_clean:\n",
    "    for w in s:\n",
    "        flat.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aprovechamos que tenemos una lista de palabras para poder contar su ocurrencia\n",
    "count = dict()\n",
    "for w in flat:\n",
    "    if w not in count:\n",
    "        count[w] = 1\n",
    "    else:\n",
    "        count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuantas veces aparece la palabra \"el\" ? ---> 8\n",
    "count[\"Rodolfo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora volvemos a unir las palabras a la oración que pertenecen\n",
    "final = []\n",
    "for i,s in enumerate(last_clean):\n",
    "    a = ' '.join(last_clean[i])\n",
    "    final.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,a in enumerate(final[:27]):\n",
    "    print(str(i)+\")\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasamos las oraciones pre-procesadas a Spacy\n",
    "parsed_sents = []\n",
    "for doc in nlp.pipe(final, batch_size=10000, n_threads=4):\n",
    "    parsed_sents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diccionario con las triplas de cada palabra\n",
    "dicc_trip = dict()\n",
    "for s in parsed_sents:\n",
    "    for w in s:\n",
    "        tags = [(w.text, w.dep_, w.head.text)]\n",
    "        if str(w) not in dicc_trip:\n",
    "            dicc_trip[str(w)] = tags\n",
    "        else:\n",
    "            dicc_trip[str(w)] += (tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de listas con tuplas de la forma (palabra, Pos)\n",
    "pos = []\n",
    "for s in parsed_sents:\n",
    "    aux=[]\n",
    "    for w in s:\n",
    "        aux.append((w, w.pos_))\n",
    "    pos.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todas las triplas relacionadas con \"el\"\n",
    "dicc_trip[\"el\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dicc_trip.items():\n",
    "    aux = []\n",
    "    for fst,snd,trd in value:\n",
    "        a = snd+'.'+trd\n",
    "        aux.append(a)\n",
    "        dicc_trip[key] = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_trip[\"el\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de etiquetas para cada oración\n",
    "tag_sents = [[(token.text,token.tag_,token.pos_) for token in parsedEx] for parsedEx in parsed_sents]\n",
    "\n",
    "# Lista de triplas de dependencias para cada oración\n",
    "#dep_trips = [[(token.orth_, token.dep_, token.head.orth_) for token in parsedEx] for parsedEx in parsed_sents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final ---> lista de oraciones procesadas\n",
    "# tag_sents ---> tags de cada oración\n",
    "# dep_trips ---> triplas de dep de cada oración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntamos todo\n",
    "#triplas = [tri for tri in zip(final, tag_sents, dep_trips)]\n",
    "\n",
    "#ahora tenemos algo de la forma (oración, tags, tripla de dependencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_clean = []\n",
    "for i,a in enumerate(tag_sents):\n",
    "    for fst,snd,trd in a:\n",
    "        aux = snd.split(\"|\")\n",
    "        tgs = []\n",
    "        for x in aux:\n",
    "            y = x.split(\"=\")\n",
    "            if len(y) == 1:\n",
    "                tgs.append(y[0])\n",
    "            else:\n",
    "                tgs.append((y[0], y[1]))\n",
    "        tags.append((fst, tgs, trd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_info_word = []\n",
    "for a in tag_sents:\n",
    "    for fst,snd,trd in a:\n",
    "        aux = snd.split(\"|\")\n",
    "        clean_info_word.append((fst,aux,trd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora tenemos una lista de triplas, donde el primer elem es la palabra, el segundo sus tags y el tercero su PoS\n",
    "print(clean_info_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dicc_forward = defaultdict(list)\n",
    "dicc_backward = defaultdict(list)\n",
    "for tup in pos:\n",
    "    for j,(fst,snd) in enumerate(tup):\n",
    "        if j != 0:\n",
    "            dicc_backward[fst.text].append((tup[j-1][0].text, tup[j-1][1]))\n",
    "        else:\n",
    "            dicc_backward[fst.text].append((\"[W.Start]\", \"[T.Start]\"))\n",
    "        if j != len(tup)-1:\n",
    "            dicc_forward[fst.text].append((tup[j+1][0].text, tup[j+1][1]))\n",
    "        else:\n",
    "            dicc_forward[fst.text].append((\"[W.End]\", \"[T.End]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_backward[\"Clara\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_info_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_backward[\"Clara\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando el diccionario\n",
    "dicc = dict()\n",
    "#features = dict()\n",
    "for fst,snd,trd in clean_info_word:\n",
    "    features = defaultdict(int)\n",
    "    pos = 'PoS__' + trd\n",
    "    if fst not in dicc and count[fst] > 2:      #si la palabra es nueva y ocurre mas de n veces -> la agrego\n",
    "        dicc[fst.lower()] = features\n",
    "        features[pos] = 1\n",
    "        #features['LowerCase'] = str(fst.islower)\n",
    "        for i in snd:\n",
    "            features[i] = 1\n",
    "        for i in dicc_trip[fst]:                 #diccionario de triplas\n",
    "            features[i] = 1\n",
    "        for i in dicc_backward[fst]:\n",
    "            features[i[0]+\"-1\"] += 1\n",
    "            features[i[1]+\"-1\"] += 1\n",
    "        for i in dicc_forward[fst]:\n",
    "            features[i[0]+\"+1\"] += 1\n",
    "            features[i[0]+\"+1\"] += 1\n",
    "            \n",
    "            \n",
    "    elif fst in dicc:                        #la palabra ya se encuentra -> actualizo el dicc\n",
    "        has_it = dicc[fst]\n",
    "        if pos in has_it:\n",
    "            has_it[pos] += 1\n",
    "        else:\n",
    "            has_it[pos] = 1\n",
    "        for tag in snd:\n",
    "            if tag in has_it:\n",
    "                has_it[tag] += 1\n",
    "            else:\n",
    "                has_it[tag] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_backward[\"Clara\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_forward[\"Clara\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count[\"Clara\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora preparamos la lista una lista de diccs para vectorizar\n",
    "#Tambien vamos a guardar las palabras que vamos a vectorizar para poder recuperarlas mas tarde\n",
    "lista_para_vectorizar = []\n",
    "check_list = []\n",
    "for key, value in dicc.items():\n",
    "    check_list.append(key)\n",
    "    lista_para_vectorizar.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corroboramos la correspondencia\n",
    "for i,w in enumerate(check_list):\n",
    "    a = lista_para_vectorizar[i]\n",
    "    #print(w + \" -->\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizado\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(lista_para_vectorizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#Reducción de dimensionalidad\n",
    "svd = TruncatedSVD(n_components=47, n_iter=7, random_state=42)\n",
    "svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalización\n",
    "import sklearn\n",
    "Y = sklearn.preprocessing.normalize(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = km.predict(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clusters obtenidos\n",
    "clusters = defaultdict(set)\n",
    "for i, label in enumerate(labels):\n",
    "    clusters[label].add(check_list[i])\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"dict_cluster\"\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
